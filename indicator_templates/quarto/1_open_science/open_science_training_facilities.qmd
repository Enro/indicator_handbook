# Open science training facilities {#open-science-training-facilities .unnumbered}

History (please fill out in reverse chronological order, latest revision on top):

|         |               |                              |                                             |
|---------|---------------|------------------------------|---------------------------------------------|
| Version | Revision date | Revision                     | Author                                      |
| 1.0     | 2023-02-07    | Finalised indicator template | T.P. Willemse, V.A. Traag, Thed van Leeuwen |

## Description

Open Science training is mostly organized at the institutional and even faculty level. This is often organized by the university or faculty libraries when it comes to scholarly publishing, that is, Open Access publishing. When it relates to research data, the support is nowadays in many institutions organized by installing data stewards, who take care of data management in general, data management plans and the construction of these, and the sharing of research data according to the FAIR principles.

So, the creation of metrics should take into consideration what elements of open science practices are being selected for training staff members. Given this potential diversity, and the absence of having generic or systematic data sources at hand for this aspect of open science, the data need to be collected by qualitative means, that is going through websites of institutions and /or faculties in order to identify open science training facilities.

The potential indicators mentioned are:

-   Number/% of institutions that offer OS training.
-   Number/% of national OS training initiatives.
-   Breadth of OS training (all OS areas, only limited to specific areas)
-   Duration of OS training (hour/day/week)

This indicator set can perhaps best be collected with the information regarding open science support facilities, as training can be seen as part of such a support system.

## Metrics

### Number/% of institutions that offer OS training.

This metric can be constructed by searching through university and faculty level websites to identify the presence of open science training facilities, and when present identify what the training facility actually relates to, what kind of activity in open science is being supported by offering training facilities. If one compares that to the total number of institutions and the faculties therein, with the result of the search for open science training facilities, one can create a number and share comparing the ones that have such training facilities in place, to the total number.

#### Measurement.

Following from the above, if one compares that to the total number of institutions and the faculties therein, with the result of the search for open science training facilities, one can create a number and share comparing the ones that have such facilities in place, to the total number.

Potential problems and limitations are:

-   This is a time-consuming effort, as one has to go through many websites, in particular when one is dealing with a large country, with many institutions, not necessarily limited to universities, but for example in the case of Germany, many publicly funded research institutions under the flag of Max Planck, Leibniz, Helmholtz, etc.
-   This kind of information might not be present on the website (yet), and as such create an underestimation of the actual situation.
-   How does one compare the breadth of the open science training, here one only aims at the presence, but the range of facilities to train staff members on open science practices is here not yet included.

##### *Existing data sources:*

###### &lt;data source name&gt;

There are presently no generic and systematic data sources that contain such information, so if one wants to have insight in this aspect of open science development, it has to be collected manually.

##### *Existing methodologies*

###### &lt;methodology name&gt;

If the methodology is not automated / automatable: Has data been gathered before using this technique (e.g., a specific survey instrument) before? If so, please describe earlier data collection efforts. Can the data gathering be easily done again for more current information? What needs to be adapted/changed for the data gathering to be current?

Regarding the methodology, this is as described above. One simply takes the number of institutions and compares that to the number of institutions that have open science training facilities in place.

Given the absence of data sources for this metric, automation is currently not possible.

### Number/% of national OS training initiatives.

This metric can be constructed by searching through policy initiatives that support open science practices on the national level and identify the training element out of the total of information collected.

#### Measurement.

Following from the above, if one compares that to the total number of countries, with the result of the search for open science training facilities at country level, one can create a number and share comparing the ones that have such training facilities in place to the total number.

Potential problems and limitations are:

-   This is a time-consuming effort, as one has to go through many websites, in particular with respect to the definition of ‘national’. In some countries, governance of science is not only a national issue, but might in the case of a federal government, also have to deal with federal initiative regarding the support of open science practices.
-   A similar issue relates to the presence of funders, and their support of open science practices, these are also considered national research funding agencies, and how do these relate to the abovementioned issue of the ‘national’ level.
-   How does one compare the breadth of the open science support, here one only aims at the presence, but the range of facilities to support open science is here not yet included.

##### *Existing data sources:*

###### &lt;data source name&gt;

There are presently no generic and systematic data sources that contain such information on support facilities (as there are for open access or open science mandates), so if one wants to have insight in this aspect of open science development, it has to be collected manually.

##### *Existing methodologies*

###### &lt;methodology name&gt;

If the methodology is not automated / automatable: Has data been gathered before using this technique (e.g., a specific survey instrument) before? If so, please describe earlier data collection efforts. Can the data gathering be easily done again for more current information? What needs to be adapted/changed for the data gathering to be current?

Regarding the methodology, this is as described above. One simply takes the number of countries and compares that to the number of countries that have open science support facilities in place.

Given the absence of data sources for this metric, automation is currently not possible.

### Breadth of open science training (all open science areas, only limited to specific areas).

This metric can be constructed from the first metric proposed here, namely by making an inventory of the diverse types of open science training facilities (publishing, research data, peer review, pre-registration, registered reports, etc.).

#### Measurement.

Following from the above, if one compares that to the total number of institutions, with the result of the search for open science training facilities at institutional level, one can create a number and share comparing the ones that have such training facilities in place regarding the diversity of open science support facilities, to the total number of institutions.

Potential problems and limitations are:

-   This is a time-consuming effort, as one has to go through many websites, on both overall institution level, as well as on faculty level.
-   One has to decide upon a common denominator, the support for open science practices that are frequent across the system, but how does one deal with discipline specific support facilities that are not common?
-   How does one include the various stages of development regarding open science across faculties into this metric? Some scholarly disciplines do have a different perspective on open science practices, e.g., regarding open access publishing of books in the humanities, or regarding qualitative research data in some parts of the social sciences, e.g., anthropology).

##### *Existing data sources:*

###### &lt;data source name&gt;

There are presently no generic and systematic data sources that contain such information on support facilities and the variety of open science practices therein, so if one wants to have insight in this aspect of open science development, it has to be collected manually. This could be combined with the search mentioned in relation to the first metric.

##### *Existing methodologies*

###### &lt;methodology name&gt;

If the methodology is not automated / automatable: Has data been gathered before using this technique (e.g., a specific survey instrument) before? If so, please describe earlier data collection efforts. Can the data gathering be easily done again for more current information? What needs to be adapted/changed for the data gathering to be current?

Regarding the methodology, this is as described above. One simply takes the number of institutions and compares that to that to the number of institutions that have support on specific aspects of open science practices in place (e.g., on open access publishing, or on data storage and data sharing).

Given the absence of data sources for this metric, automation is currently not possible.

### Duration of open science training (hour/day/week).

This metric can be constructed in a similar manner as the first metric proposed here, namely by looking at websites for what is offered as open science practice training facilities and collect the duration of such training facilities.

#### Measurement.

Following from the above, one can create overviews of the duration of open science training facilities, distributed over various aspects of open science practices, and compare these for example across institutions.

Potential problems and limitations are:

-   This is a time-consuming effort, as one has to go through many websites, on both overall institution level, as well as on faculty level.
-   Her one has to work with what is offered as open science training facilities, one does not have any idea of what is being ‘consumed‘ from the offered training facilities?
-   An issue that popped up previously, one has to be aware of the fact that perhaps not all of the open science support facilities are (yet) online visible on websites.

##### *Existing data sources:*

###### &lt;data source name&gt;

There are presently no generic and systematic data sources that contain such information on the duration of open science support facilities, so if one wants to have insight in this aspect of open science support, it has to be collected manually. This could be combined with the search mentioned in relation to the first metric and third metric.

##### *Existing methodologies*

###### &lt;methodology name&gt;

If the methodology is not automated / automatable: Has data been gathered before using this technique (e.g., a specific survey instrument) before? If so, please describe earlier data collection efforts. Can the data gathering be easily done again for more current information? What needs to be adapted/changed for the data gathering to be current?

Regarding the methodology, this is as described above. One simply takes the number of institutions and compares that to that to the number of institutions that have training facilities on specific aspects of open science practices and its duration in place (e.g., length/duration of the courses on open access publishing, or on data storage and data sharing).

Given the absence of data sources for this metric, automation is currently not possible.

## Known correlates

\[Add text here\]

## Notes

\[Add text here\]

## References

\[Add Zotero bibliography here\]
