# Prevalence of open science support {#prevalence-of-open-science-support .unnumbered}

History (please fill out in reverse chronological order, latest revision on top):

|         |               |                              |                                             |
|---------|---------------|------------------------------|---------------------------------------------|
| Version | Revision date | Revision                     | Author                                      |
| 1.0     | 2023-02-07    | Finalised indicator template | T.P. Willemse, V.A. Traag, Thed van Leeuwen |

## Description

Open Science support is mostly organized at the institutional and even faculty level. This is often organized by the university or faculty libraries when it comes to scholarly publishing, that is, Open Access publishing. Here the support consists mostly of suggesting journals, helping with license details, etc. When it relates to research data, the support is nowadays in many institutions organized by installing data stewards, who first of all take care of data management in general, after which the delicate issue of open research data according to the FAIR principles appears as an issue.

So, the creation of metrics should take into consideration what elements of open science practices are being selected for support. Given this potential diversity, and the absence of having generic or systematic data sources at hand for this aspect of open science, the data need to be collected by qualitative means, that is going through websites of institutions and /or faculties in order to identify open science support facilities.

The potential indicators mentioned are:

-   Number/% of institutions that offer OS support.
-   Number/% of national OS support initiatives.
-   Breadth of OS support (all OS areas, only limited to specific areas)
-   Duration of OS support (hour/day/week)

## Metrics

### Number/% of institutions that offer OS support

This metric can be constructed by searching through university and faculty level websites to identify the presence of open science support facilities, and when present identify what the support facility actually relates to, what kind of activity in open science is being supported. If one compares that to the total number of institutions and the faculties therein, with the result of the search for open science support facilities, one can create a number and share comparing the ones that have such facilities in place to the total number.

#### Measurement.

Following from the above, if one compares that to the total number of institutions and the faculties therein, with the result of the search for open science support facilities, one can create a number and share comparing the ones that have such facilities in place to the total number.

Potential problems and limitations are:

-   This is a time-consuming effort, as one has to go through many websites, in particular when one is dealing with a large country, with many institutions, not necessarily limited to universities, but for example in the case of Germany, many publicly funded research institutions under the flag of Max Planck, Leibniz, Helmholtz, etc.
-   This kind of information might not be present on the website (yet), and as such create an underestimation of the actual situation.
-   How does one compare the breadth of the open science support, here one only aims at the presence, but the range of facilities to support open science is here not yet included.

##### Existing data sources:

###### &lt;data source name&gt;

There are presently no generic and systematic data sources that contain such information, so if one wants to have insight in this aspect of open science development, it has to be collected manually.

##### Existing methodologies

###### &lt;methodology name&gt;

Regarding the methodology, this is as described above. One simply takes the number of institutions and compares that to the number of institutions that have open science support facilities in place.

Given the absence of data sources for this metric, automation is currently not possible.

### Number/% of national OS support initiatives.

This metric can be constructed by searching through policy initiatives that support open science practices on the national level.

#### Measurement.

Following from the above, if one compares that to the total number of countries, with the result of the search for open science support facilities at country level, one can create a number and share comparing the ones that have such facilities in place to the total number.

Potential problems and limitations are:

-   This is a time-consuming effort, as one has to go through many websites, in particular with respect to the definition of ‘national’. In some countries, governance of science is not only a national issue, but might in the case of a federal government, also have to deal with federal initiative regarding the support of open science practices.
-   A similar issue relates to the presence of funders, and their support of open science practices, these are also considered national research funding agencies, and how do these relate to the abovementioned issue of the ‘national’ level.
-   How does one compare the breadth of the open science support, here one only aims at the presence, but the range of facilities to support open science is here not yet included.

##### Existing data sources:

###### &lt;data source name&gt;

There are presently no generic and systematic data sources that contain such information on support facilities (as there are for open access or open science mandates), so if one wants to have insight in this aspect of open science development, it has to be collected manually.

##### Existing methodologies

###### &lt;methodology name&gt;

Regarding the methodology, this is as described above. One simply takes the number of countries and compares that to the number of countries that have open science support facilities in place.

Given the absence of data sources for this metric, automation is currently not possible.

### Breadth of open science support (all open science areas, only limited to specific areas).

This metric can be constructed from the first metric proposed here, namely by making an inventory of the diverse types of open science support facilities (publishing, research data, peer review, pre-registration, registered reports, etc.).

#### Measurement.

Following from the above, if one compares that to the total number of institutions, with the result of the search for open science support facilities at institutional level, one can create a number and share comparing the ones that have such facilities in place regarding the diversity of open science support facilities to the total number of institutions.

Potential problems and limitations are:

-   This is a time-consuming effort, as one has to go through many websites, on both overall institution level, as well as on faculty level.
-   One has to decide upon a common denominator, the support for open science practices that are frequent across the system, but how does one deal with discipline specific support facilities that are not common?

##### Existing data sources:

###### &lt;data source name&gt;

There are presently no generic and systematic data sources that contain such information on support facilities and the variety of open science practices therein, so if one wants to have insight in this aspect of open science development, it has to be collected manually. This could be combined with the search mentioned in relation to the first metric.

##### Existing methodologies

###### &lt;methodology name&gt;

Regarding the methodology, this is as described above. One simply takes the number of institutions and compares that to that to the number of institutions that have support on specific aspects of open science practices in place (e.g., on open access publishing, or on data storage and data sharing).

Given the absence of data sources for this metric, automation is currently not possible.

### Duration of open science support (hour/day/week).

This metric can be constructed in a similar manner as the first metric proposed here, namely by looking at websites for what is offered as open science practice support facilities and collect the duration of such facilities.

#### Measurement.

Following from the above, one can create overviews of the duration of open science support facilities, distributed over various aspects of open science practices, and compare these for example across institutions.

Potential problems and limitations are:

-   This is a time-consuming effort, as one has to go through many websites, on both overall institution level, as well as on faculty level.
-   Her one has to work with what is offered as open science support facilities; one does not have any idea of what is being ‘consumed‘ from the offered facilities?
-   An issue that popped up previously, one has to be aware of the fact that perhaps not all of the open science support facilities are (yet) online visible on websites.

##### Existing data sources:

###### &lt;data source name&gt;

There are presently no generic and systematic data sources that contain such information on the duration of open science support facilities, so if one wants to have insight in this aspect of open science support, it has to be collected manually. This could be combined with the search mentioned in relation to the first metric and third metric.

##### Existing methodologies

###### &lt;methodology name&gt;

Regarding the methodology, this is as described above. One simply takes the number of institutions and compares that to that to the number of institutions that have support on specific aspects of open science practices and its duration in place (e.g., length/duration of the courses on open access publishing, or on data storage and data sharing).

Given the absence of data sources for this metric, automation is currently not possible.

## Known correlates

\[Add text here\]

## Notes

\[Add text here\]

## References

\[Add Zotero bibliography here\]
