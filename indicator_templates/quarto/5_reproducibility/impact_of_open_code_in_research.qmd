# Impact of open code in research

History (please fill out in reverse chronological order, latest revision
on top):

|         |               |                              |                           |
| ------- | ------------- | ---------------------------- | ------------------------- |
| Version | Revision date | Revision                     | Author                    |
| 1.1     | 2023-05-11    | First Draft                  | Petros Stavropoulos       |
| 1.0     | 2023-02-07    | Finalised indicator template | T.P. Willemse, V.A. Traag |

## Description

The impact of open code in research refers to the effect of making
research code openly accessible and reusable on the scientific community
and society as a whole. This indicator aims to capture the extent to
which open code practices are being adopted in research and their
potential impact on the scientific output and societal benefits. The
indicator can be used to assess the level of openness and accessibility
of research code within a specific scientific community or field and to
identify potential barriers or incentives for the adoption of open code
practices. It can also be used to track the reuse and impact of open
code on subsequent research, as well as to evaluate the effectiveness of
policies and initiatives promoting open code practices.

## Metrics

### FWCI for publication that have introduced open code

This metric calculates the Field-Weighted Citation Impact (FWCI) for
publications that have introduced open code. FWCI is a measure of the
citation impact of a publication, adjusted for differences in citation
practices across scientific fields. The FWCI for publications that have
introduced open code can indicate the potential impact of open code
practices on the visibility and influence of research findings.

One limitation of this metric is that it may not capture the impact of
open code practices on research findings that are not directly related
to the introduction of open code. Additionally, the use of FWCI as a
metric has been criticized for its potential biases and limitations,
such as the inability to fully account for differences in research
quality or the influence of non-citation-based impact measures.
Therefore, it is recommended to use this metric in conjunction with
other metrics to obtain a more comprehensive assessment of the impact of
open code practices on research output.

#### Measurement.

To measure this metric, we can start by identifying publications that
have introduced open code. This can be done by searching for
publications that have introduced open code in code repositories such as
GitHub, GitLab, or Bitbucket, or by using automatic tools that extract
introduced code/software from publications such as the PathOS work in
Task 2.5. Once we have identified the relevant publications, we need to
determine their discipline to calculate their FWCI score. Indicatively,
we can use toolkits like SciNoBo or Elsevier’s SciVal, which provide
discipline-specific citation impact metadata. Alternatively, we can also
use the Web of Science and Scopus classifications to determine the
disciplines of the relevant publications.

One potential limitation of this approach is that not all open code may
be registered in code repositories, making it challenging to identify
all relevant publications. Additionally, the accuracy of the FWCI score
may be affected by the availability and quality of citation data in
different scientific fields. Therefore, it is important to carefully
consider the potential biases and limitations of the data sources and
methodologies used to measure this metric.

##### Existing datasources:

###### \<datasource name\>

\[Add text here\]

##### Existing methodologies

###### PathOS work in Task 2.5

This is an automated tool used to identify code/software mentioned in
the text of publications and extract metadata associated with them, such
as name, version, license, URLs etc. This tool can also classify whether
the code/software has been introduced by the authors of the publication.

To measure the FWCI for publications that have introduced open
code/software, the tool can be used to identify relevant publications
that have introduced code/software, along with code repositories in
GitHub, GitLab, or Bitbucket where the code/software is openly located.

###### SciNoBo toolkit

The SciNoBo toolkit can be used to classify scientific publications into
specific fields of science, which can then be used to calculate their
FWCI score. The tool utilizes the citation-graph of a publication and
its references to identify its discipline and assign it to a specific
Field-of-Science (FoS) taxonomy. The classification system of
publications is based on the structural properties of a publication and
its citations and references organized in a multilayer network.

To use SciNoBo to measure this metric, we would need to feed the
publications identified from the PathOS work in Task 2.5 tool into the
SciNoBo toolkit. This would result in the classification of the
publications into specific fields of science and their corresponding
FWCI score.

### FWCI for publication that have (re)used open code

This metric calculates the Field-Weighted Citation Impact (FWCI) for
publications that have (re)used open code. FWCI is a measure of the
citation impact of a publication, adjusted for differences in citation
practices across scientific fields. The FWCI for publications that have
introduced open code can indicate the potential impact of open code
practices on the visibility and influence of research findings.

One limitation of this metric is that it may not capture the impact of
open code practices on research findings that are not directly related
to the (re)use of open code. Additionally, the use of FWCI as a metric
has been criticized for its potential biases and limitations, such as
the inability to fully account for differences in research quality or
the influence of non-citation-based impact measures. Therefore, it is
recommended to use this metric in conjunction with other metrics to
obtain a more comprehensive assessment of the impact of open code
practices on research output.

#### Measurement.

To measure this metric, we can start by identifying publications that
have (re)used open code. This can be done by searching for publications
that have (re)used open code in code repositories such as GitHub,
GitLab, or Bitbucket, or by using automatic tools that extract
introduced code/software from publications such as the PathOS work in
Task 2.5. Once we have identified the relevant publications, we need to
determine their discipline to calculate their FWCI score. Indicatively,
we can use toolkits like SciNoBo or Elsevier’s SciVal, which provide
discipline-specific citation impact metadata. Alternatively, we can also
use the Web of Science and Scopus classifications to determine the
disciplines of the relevant publications.

One potential limitation of this approach is that not all open code may
be registered in code repositories, making it challenging to identify
all relevant publications. Additionally, the accuracy of the FWCI score
may be affected by the availability and quality of citation data in
different scientific fields.

##### Existing datasources:

###### \<datasource name\>

\[Add text here\]

##### Existing methodologies

###### PathOS work in Task 2.5

This is an automated tool used to identify code/software mentioned in
the text of publications and extract metadata associated with them, such
as name, version, license, URLs etc. This tool can also classify whether
the code/software has been (re)used by the authors of the publication.

To measure the FWCI for publications that have (re)used open
code/software, the tool can be used to identify relevant publications
that have introduced code/software, along with code repositories in
GitHub, GitLab, or Bitbucket where the code/software is openly located.

###### SciNoBo toolkit

The SciNoBo toolkit can be used to classify scientific publications into
specific fields of science, which can then be used to calculate their
FWCI score. The tool utilizes the citation-graph of a publication and
its references to identify its discipline and assign it to a specific
Field-of-Science (FoS) taxonomy. The classification system of
publications is based on the structural properties of a publication and
its citations and references organized in a multilayer network.

To use SciNoBo to measure this metric, we would need to feed the
publications identified from the PathOS work in Task 2.5 tool into the
SciNoBo toolkit. This would result in the classification of the
publications into specific fields of science and their corresponding
FWCI score.

### Code downloads/usage counts/stars from repositories

This metric measures the number of times an open code repository has
been downloaded, used, or favourited, which can indicate the level of
interest and impact of the code on the scientific community.

However, this metric may have limitations in capturing the impact of
code that is not hosted in a public repository or downloaded through
other means, such as direct communication between researchers.

Additionally, usage counts and stars may not necessarily reflect the
quality or impact of the code, and may be influenced by factors such as
marketing and social media outreach. Therefore, it is recommended to use
this metric in conjunction with other metrics to obtain a more
comprehensive assessment of the impact of open code practices on
research output.

#### Measurement.

To measure this metric, data can be obtained from code repositories such
as GitHub, GitLab, or Bitbucket. The number of downloads, usage counts,
and stars can be extracted from the repository metadata. For example, on
GitHub, this data is available through the API or by accessing the
repository page. However, it is important to note that not all
repositories may make this information publicly available, and some may
only provide partial or incomplete usage data.

Additionally, the accuracy of the usage data may be affected by factors
such as the frequency of updates, the type of license, and the
accessibility of the code to different research communities. Therefore,
it is important to carefully consider the potential biases and
limitations of the data sources and methodologies used to measure this
metric.

The data can be computationally obtained using web scraping tools, API
queries, or by manually accessing the download/usage count/star data.

##### Existing datasources:

###### Github

GitHub is a web-based platform used for version control and
collaborative software development. It allows users to create and host
code repositories, including those for open source software and
datasets. The number of downloads, usage counts, and stars on GitHub can
be used as a metric for the impact and popularity of open datasets.

To measure this metric, we can search for the relevant repositories on
GitHub and extract the relevant download, usage, and star data. This
data can be accessed via the GitHub API, which provides programmatic
access to repository data. The API can be queried using HTTP requests,
and the resulting data can be parsed and analyzed using programming
languages such as Python.

###### GitLab

GitLab is a web-based Git repository manager that provides source code
management, continuous integration and deployment, and more. It can be
used as a data source for metrics related to the usage of open-source
software projects, including the number of downloads, stars, and forks.

To calculate the metric of code downloads/usage counts/stars from
GitLab, we need to identify the relevant repositories and extract the
relevant information. The number of downloads can be obtained by looking
at the download statistics for a particular release of the repository.
The number of stars can be obtained by looking at the number of users
who have starred the repository. The number of forks can be obtained by
looking at the number of users who have forked the repository.

To access this information, we can use the GitLab API.

###### Bitbucket

Bitbucket is a web-based Git repository hosting service that allows
users to host their code repositories, collaborate with other users and
teams, and automate their software development workflows. It can be used
as a data source for metrics related to the usage of open-source
software projects, including the number of downloads, stars, and forks.

To calculate the metric of code downloads/usage counts/stars from
Bitbucket, we need to identify the relevant repositories and extract the
relevant information. The number of downloads can be obtained by looking
at the download statistics for a particular release of the repository.
The number of stars can be obtained by looking at the number of users
who have starred the repository. The number of forks can be obtained by
looking at the number of users who have forked the repository.

To access this information, we can use the Bitbucket API, which provides
programmatic access to repository data. The API can be queried using
HTTP requests, and the resulting data can be parsed and analyzed using
programming languages such as Python.

##### Existing methodologies

###### Scrapers / REST API Clients ?

\[add text here\].

## Known correlates

\[Add text here\]

## Notes

\[Add text here\]

## References

\[Add Zotero bibliography here\]
